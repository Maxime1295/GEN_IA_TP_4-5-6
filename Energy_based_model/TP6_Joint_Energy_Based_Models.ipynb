{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each practical exercise (TP), please work in groups of two or three. Then, create a **private GitHub repository** and add me (my GitHub is **arthur-75**) to your project. Finally, share the link to your project (or TP) under  [Practical Exercises](https://docs.google.com/spreadsheets/d/1V-YKgHn71FnwjoFltDhWsPJS7uIuAh9lj6SP2DSCvlY/edit?usp=sharing) and make sure to choose your **team name** :-)\n",
    "\n",
    "# **Joint Energy-Based Models (JEM)**\n",
    "\n",
    "## **1\\. Understanding Energy-Based Models (EBMs)**\n",
    "\n",
    "### **1.1 What is an Energy-Based Model (EBM)?**\n",
    "\n",
    "An Energy-Based Model (EBM) defines a probability distribution through an energy function Eθ(x). The lower the energy, the more probable the input. the probability distribution is defined as:  \n",
    "        ![][image1]\n",
    "\n",
    "where:\n",
    "\n",
    "* Eθ​(x) is the energy function.  \n",
    "*  ![][image2] is the partition function (normalization constant).\n",
    "\n",
    "### **1.2 Joint Energy-Based Model (JEM)**\n",
    "\n",
    "JEM extends EBMs to also work as a classifier. Instead of defining only pθ​(x), it also models pθ​(y∣x), meaning:  \n",
    "        ![][image3]\n",
    "\n",
    "which enables both classification and generative modeling.\n",
    "\n",
    "We decompose the energy into two parts:\n",
    "\n",
    "1. **Classification logits**: fθ(x) (Neural Network output).  \n",
    "2. **Energy Function**: Extracted from feature representations.\n",
    "\n",
    "The probability of a class label is then:  \n",
    "            ![][image4]\n",
    "\n",
    "which is just a standard softmax classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random as random\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "device = \"mps\"#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "img_shape = (1, 28, 28)\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "lr = 1e-4\n",
    "beta1 = 0.0\n",
    "alpha = 0.1 \n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **2\\. Designing the JEM Model**\n",
    "\n",
    "### **2.1 Neural Network Structure**\n",
    "\n",
    "The model consists of:\n",
    "\n",
    "1. **CNN-based feature extractor** (convolutional layers for learning representations).  \n",
    "2. **Two outputs**:  \n",
    "   * **Energy output**: Eθ​(x), which determines sample likelihood.  \n",
    "   * **Classification logits**: fθ(x), which classifies the sample.\n",
    "\n",
    "The training objective involves:\n",
    "\n",
    "* Standard **cross-entropy loss** for classification.  \n",
    "* **Contrastive divergence** to train the energy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Swish(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "        \n",
    "class JEMClassifier(nn.Module):\n",
    "    def __init__(self, hidden_features=32):\n",
    "        super().__init__()\n",
    "        c_hid1 = hidden_features // 2\n",
    "        c_hid2 = hidden_features\n",
    "        c_hid3 = hidden_features * 2\n",
    "\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, c_hid1, kernel_size=5, stride=2, padding=4),  # [16x16]\n",
    "            Swish(),\n",
    "            nn.Conv2d(c_hid1, c_hid2, kernel_size=3, stride=2, padding=1),  # [8x8]\n",
    "            Swish(),\n",
    "            nn.Conv2d(c_hid2, c_hid3, kernel_size=3, stride=2, padding=1),  # [4x4]\n",
    "            Swish(),\n",
    "            nn.Conv2d(c_hid3, c_hid3, kernel_size=3, stride=2, padding=1),  # [2x2]\n",
    "            Swish(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc_energy = nn.Linear(c_hid3 * 4, 1) \n",
    "        self.fc_class = nn.Linear(c_hid3 * 4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn_layers(x)\n",
    "        energy = self.fc_energy(features).squeeze(-1)\n",
    "        logits = self.fc_class(features)\n",
    "        return energy, logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **2.2 Computing Energy**\n",
    "\n",
    "* In a CNN, the **feature representation** of an input x is used to compute energy:  \n",
    "  Eθ​(x)=Linear(features)\n",
    "\n",
    "where the features are extracted from the convolutional layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  **3\\. Sampling from the Model**\n",
    "\n",
    "### **3.1 Markov Chain Monte Carlo (MCMC) Sampling**\n",
    "\n",
    "Since the energy model defines an implicit probability distribution, we **cannot directly sample from it**. Instead, we use **Langevin Dynamics**, a gradient-based MCMC method.\n",
    "\n",
    "The **Langevin update step** is:  \n",
    "            ![][image5]  \n",
    "where:\n",
    "\n",
    "* α is the step size.  \n",
    "* ∇xEθ(xt) is the gradient of energy w.r.t input x.  \n",
    "* ηt ∼ N(0,σ2) is Gaussian noise for stochasticity.\n",
    "\n",
    "### **3.2 Buffer-Based Sampling**\n",
    "\n",
    "* **Replay Buffer**: Stores previously sampled images.  \n",
    "* **New Sample Initialization**:  \n",
    "  * With probability **5%**, draw a completely random sample.  \n",
    "  * With probability **95%**, draw from the buffer.\n",
    "\n",
    "The **MCMC sampling process**:\n",
    "\n",
    "1. Initialize from random noise or buffer.  \n",
    "2. Apply Langevin updates for **N steps**.  \n",
    "3. Clip values to stay within valid input range.\n",
    "\n",
    "This results in diverse and high-quality samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sampler:\n",
    "\n",
    "    def __init__(self, model, img_shape, sample_size, max_len=8192):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            model - Neural network to use for modeling E_theta\n",
    "            img_shape - Shape of the images to model\n",
    "            sample_size - Batch size of the samples\n",
    "            max_len - Maximum number of data points to keep in the buffer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.img_shape = img_shape\n",
    "        self.sample_size = sample_size # same as the batch size\n",
    "        self.max_len = max_len\n",
    "        self.examples = [(torch.rand((1,)+img_shape)*2-1) for _ in range(self.sample_size)]\n",
    "\n",
    "    def sample_new_exmps(self, steps=60, step_size=10):\n",
    "        \"\"\"\n",
    "        generates fake samples using the replay buffer and MCMC\n",
    "        \"\"\"\n",
    "        device = next(self.model.parameters()).device \n",
    "\n",
    "        ## see on the training algo - 95% of chances to get from the buffer and 5% from random noise\n",
    "        n_new = np.random.binomial(self.sample_size, 0.05)\n",
    "        rand_imgs = torch.rand((n_new,) + self.img_shape, device=device) * 2 - 1\n",
    "        old_imgs = torch.cat(random.choices(self.examples, k=self.sample_size - n_new), dim=0).to(device)\n",
    "        inp_imgs = torch.cat([rand_imgs, old_imgs], dim=0).detach()\n",
    "\n",
    "        ## MCMC sampling\n",
    "        inp_imgs = Sampler.generate_samples(self.model, inp_imgs, steps=steps, step_size=step_size)\n",
    "\n",
    "        ## update buffer\n",
    "        self.examples = list(inp_imgs.cpu().chunk(self.sample_size, dim=0)) + self.examples\n",
    "        self.examples = self.examples[:self.max_len]\n",
    "        return inp_imgs\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_samples(model, inp_imgs, steps=60, step_size=10, return_img_per_step=False):\n",
    "        \"\"\"\n",
    "        MCMC step\n",
    "        \"\"\"\n",
    "        device = inp_imgs.device\n",
    "        is_training = model.training\n",
    "        model.eval()\n",
    "\n",
    "        #before MCMC: freeze the model\n",
    "        #we are only interested in the gradients of the input.\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "        inp_imgs.requires_grad = True\n",
    "\n",
    "        had_gradients_enabled = torch.is_grad_enabled()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        noise = torch.randn(inp_imgs.shape, device=device) \n",
    "\n",
    "        imgs_per_step = []\n",
    "\n",
    "        for _ in range(steps): #the MCMC interation steps\n",
    "            noise.normal_(0, 0.005)\n",
    "            inp_imgs.data.add_(noise.data)\n",
    "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
    "\n",
    "            out_imgs = -model(inp_imgs)[0] #only modification from the usual EBM sampler\n",
    "            out_imgs.sum().backward()\n",
    "            inp_imgs.grad.data.clamp_(-0.03, 0.03)\n",
    "\n",
    "            inp_imgs.data.add_(-step_size * inp_imgs.grad.data)\n",
    "            inp_imgs.grad.detach_()\n",
    "            inp_imgs.grad.zero_()\n",
    "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
    "\n",
    "            if return_img_per_step:\n",
    "                imgs_per_step.append(inp_imgs.clone().detach())\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "        model.train(is_training)\n",
    "        torch.set_grad_enabled(had_gradients_enabled)\n",
    "\n",
    "        if return_img_per_step:\n",
    "            return torch.stack(imgs_per_step, dim=0)\n",
    "        else:\n",
    "            return inp_imgs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **4\\. Loss Function**\n",
    "\n",
    "The loss function combines:\n",
    "\n",
    "1. **Classification loss**: Cross-entropy for supervised learning.  \n",
    "        ![][image6]  \n",
    "2. **Contrastive divergence loss**: Encourages separation between real and generated samples.\n",
    "\n",
    "   ![][image7]\n",
    "\n",
    "3. **Regularization**: Energy stability constraint.  \n",
    "        ![][image8]\n",
    "\n",
    "### **Total Loss**\n",
    "\n",
    "![][image9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "val_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = JEMClassifier(hidden_features=32).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.97)\n",
    "sampler = Sampler(model, img_shape=(1, 28, 28), sample_size=batch_size)\n",
    "\n",
    "\n",
    "def jem_loss(energy_real, energy_fake, logits, labels, alpha):\n",
    "    min_batch_size = min(energy_real.size(0), energy_fake.size(0))\n",
    "    energy_real = energy_real[:min_batch_size]\n",
    "    energy_fake = energy_fake[:min_batch_size]\n",
    "\n",
    "    reg_loss = alpha * (energy_real ** 2 + energy_fake ** 2).mean()\n",
    "    cdiv_loss = energy_fake.mean() - energy_real.mean()\n",
    "\n",
    "    class_loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "\n",
    "    total_loss = reg_loss + cdiv_loss + class_loss\n",
    "    return total_loss, reg_loss, cdiv_loss, class_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **5\\. Training Process**\n",
    "\n",
    "1. **Data Preprocessing**: Normalize input images to \\[-1,1\\].  \n",
    "2. **Forward Pass**:  \n",
    "   * Compute **energy** and **logits** for real images.  \n",
    "   * Generate **fake images** via MCMC.  \n",
    "   * Compute **energy** for fake images.  \n",
    "3. **Loss Computation**:  \n",
    "   * Compute classification, contrastive divergence, and regularization losses.  \n",
    "   * Backpropagate and update weights.  \n",
    "4. **Buffer Update**: Store MCMC samples for later use.  \n",
    "5. **Repeat for all epochs**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss, reg_loss, cdiv_loss, class_loss = 0, 0, 0, 0\n",
    "\n",
    "    for real_imgs, labels in train_loader:\n",
    "        real_imgs, labels = real_imgs.to(device), labels.to(device)\n",
    "\n",
    "        small_noise = torch.randn_like(real_imgs) * 0.005\n",
    "        real_imgs = real_imgs + small_noise\n",
    "        real_imgs = real_imgs.clamp(min=-1.0, max=1.0)\n",
    "\n",
    "        fake_imgs = sampler.sample_new_exmps(steps=60, step_size=10)\n",
    "\n",
    "        energy_real, logits_real = model(real_imgs)\n",
    "        energy_fake, _ = model(fake_imgs)\n",
    "\n",
    "        loss, r_loss, c_loss, cls_loss = jem_loss(\n",
    "            energy_real, energy_fake, logits_real, labels, alpha\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        reg_loss += r_loss.item()\n",
    "        cdiv_loss += c_loss.item()\n",
    "        class_loss += cls_loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for val_imgs, val_labels in val_loader:\n",
    "            val_imgs, val_labels = val_imgs.to(device), val_labels.to(device)\n",
    "            _, logits_val = model(val_imgs)\n",
    "            predictions = torch.argmax(logits_val, dim=1)\n",
    "            correct += (predictions == val_labels).sum().item()\n",
    "            total += val_labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_loss / len(train_loader):.4f}, Reg Loss: {reg_loss / len(train_loader):.4f}, \"\n",
    "          f\"CDiv Loss: {cdiv_loss / len(train_loader):.4f}, Class Loss: {class_loss / len(train_loader):.4f}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **6\\. Generating Images**\n",
    "\n",
    "* Use **Langevin Dynamics** for **N=2000** steps to get clear images.  \n",
    "* Use replay buffer to improve diversity.  \n",
    "* Visualize generated images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_images_with_sampler(model, sampler, num_images=16, steps=60, step_size=10):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_imgs = sampler.sample_new_exmps(steps=steps, step_size=step_size)\n",
    "        generated_imgs = generated_imgs[:num_images].detach() \n",
    "\n",
    "    return generated_imgs.cpu()\n",
    "\n",
    "\n",
    "def plot_generated_images(images, num_cols=4):\n",
    "    num_images = images.size(0)\n",
    "    num_rows = (num_images + num_cols - 1) // num_cols\n",
    "\n",
    "    images = (images + 1) / 2 \n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 2, num_rows * 2))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < num_images:\n",
    "            img = images[i].squeeze(0).numpy()\n",
    "            ax.imshow(img, cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "        else:\n",
    "            ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "num_images = 16\n",
    "generated_images = generate_images_with_sampler(model, sampler, num_images=num_images, steps=2000, step_size=0.1)\n",
    "\n",
    "plot_generated_images(generated_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
